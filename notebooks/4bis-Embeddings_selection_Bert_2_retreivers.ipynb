{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8acd57c9-aab7-4ec0-bf14-d37c21f31093",
   "metadata": {},
   "source": [
    "# Chargement des données textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8111746a-89c5-471b-a7c8-9e63e8eee4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4132, 3)\n",
      "************************************************************************************************************************\n",
      "Source Etablissement recevant des travailleurs: 299\n",
      "Source Réglementation sécurité et incendie. Etablissement recevant du public: 44\n",
      "Source Règlement de sécurité contre les risques d'incendie et de panique. Etablissements recevant du public: 1161\n",
      "Source Code de la construction et de l'habitation: 2628\n",
      "************************************************************************************************************************\n",
      "['Etablissement recevant des travailleurs', \"Obligations du maître d'ouvrage pour la conception des lieux de travail  (Articles R4211-1 à R4217-2)\", 'Sécurité des lieux de travail (Articles R4214-1 à R4214-28)', 'Caractéristiques des bâtiments (Articles R4214-1 à R4214-8)', 'Article R4214-4', 'Créé par Décret n°2008-244 du 7 mars 2008 - art', '(V)', '']\n",
      "Les surfaces des planchers, des murs et des plafonds sont conçues de manière à pouvoir être nettoyées ou ravalées en vue d obtenir des conditions d hygiène appropriées.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def correct_text(text):\n",
    "    text = re.sub(r'([a-zA-Z])([A-Z])', r'\\1 \\2', text) \n",
    "    text = re.sub(r'article(\\d+)', r'article \\1', text) \n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  \n",
    "    text = re.sub(r'([LR])\\.\\s*(\\d[-\\d]*)', r'\\1.\\2', text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    return text\n",
    "    \n",
    "df = pd.read_excel(\"./base_de_donnees.xlsx\") \n",
    "\n",
    "df['Contenu'] = df['Contenu'].apply(correct_text)\n",
    "texts_ = df['Contenu'].tolist()\n",
    "df['Contexte'] = df['Source'] + '.x. ' + df['Contexte']\n",
    "contexts = df['Contexte'].map(lambda x: x.replace('.x.', '.')).tolist()\n",
    "print(df.shape)\n",
    "print('*' * 120)\n",
    "for source in df.Source.unique():\n",
    "    print(f\"Source {source}: {len(df[df['Source']==source])}\")\n",
    "\n",
    "print('*' * 120)\n",
    "print(contexts[24].split('. '))\n",
    "print(texts_[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602cb1c-7f32-4532-b3f3-d116a114e027",
   "metadata": {},
   "source": [
    "# GPU Actif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50f3586e-56e9-47ef-9521-f9c1bdcf2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu111\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab431311-5a0c-4510-b1e1-381dc31da2af",
   "metadata": {},
   "source": [
    "# Tokenizer & Embedding et application aux Textes et Contextes\n",
    "- 4 modèles de tokenization/embedding utilisés: 'Flaubert', 'Camembert', 'Bert', 'DistilBert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43b9e72c-af26-4473-a615-e777f0ab7d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: 4132: 100%|████████████████████████████████████████████████████████████▉| 4132/4133 [01:17<00:00, 53.16it/s]\n",
      "Text embeddings done.\n",
      "processed: 4132: 100%|████████████████████████████████████████████████████████████▉| 4132/4133 [01:11<00:00, 57.98it/s]\n",
      "Context embeddings done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import FlaubertTokenizer, FlaubertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "models_names = ['flaubert/flaubert_base_cased', \"camembert-base\", \"bert-base-multilingual-cased\", 'distilbert-base-uncased']\n",
    "models_suffix = ['Flaubert', 'Camembert', 'Bert', 'DistilBert']\n",
    "model_index = 0\n",
    "model_name = models_names[model_index]\n",
    "model_suffix = models_suffix[model_index]\n",
    "\n",
    "tokenizer = globals()[f'{model_suffix}Tokenizer'].from_pretrained(model_name)\n",
    "model = globals()[f'{model_suffix}Model'].from_pretrained(model_name).to(device)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu()  \n",
    "\n",
    "\n",
    "def process_embeddings(data):\n",
    "    embeddings = []\n",
    "    with tqdm(total=int(len(data))+1,file=sys.stdout) as prog_bar:\n",
    "        for i in range(len(data)):\n",
    "            embeddings.append(get_embeddings(data[i]))\n",
    "            prog_bar.set_description('processed: %d' % (1 + i))\n",
    "            prog_bar.update(1)            \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "text_embeddings = process_embeddings(texts_)\n",
    "print(\"Text embeddings done.\")\n",
    "\n",
    "context_embeddings = process_embeddings(contexts)\n",
    "print(\"Context embeddings done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c2a8d-8307-49ca-b7d1-25007497ba38",
   "metadata": {},
   "source": [
    "# Aperçu de df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1e103d0-ce29-4643-b02c-40683b4947db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Contexte</th>\n",
       "      <th>Contenu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Etablissement recevant des travailleurs</td>\n",
       "      <td>Etablissement recevant des travailleurs.x. Obl...</td>\n",
       "      <td>Les dispositions du présent titre déterminent,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Etablissement recevant des travailleurs</td>\n",
       "      <td>Etablissement recevant des travailleurs.x. Obl...</td>\n",
       "      <td>Pour l application du présent titre, on entend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Etablissement recevant des travailleurs</td>\n",
       "      <td>Etablissement recevant des travailleurs.x. Obl...</td>\n",
       "      <td>Le maître d ouvrage élabore et transmet aux ut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Etablissement recevant des travailleurs</td>\n",
       "      <td>Etablissement recevant des travailleurs.x. Obl...</td>\n",
       "      <td>Le dossier de maintenance des lieux de travail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Etablissement recevant des travailleurs</td>\n",
       "      <td>Etablissement recevant des travailleurs.x. Obl...</td>\n",
       "      <td>Le dossier de maintenance des lieux de travail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>Code de la construction et de l'habitation</td>\n",
       "      <td>Code de la construction et de l'habitation.x. ...</td>\n",
       "      <td>Pour leur application à Saint-Pierre-et-Miquel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Code de la construction et de l'habitation</td>\n",
       "      <td>Code de la construction et de l'habitation.x. ...</td>\n",
       "      <td>Pour leur application à Saint-Pierre-et-Miquel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>Code de la construction et de l'habitation</td>\n",
       "      <td>Code de la construction et de l'habitation.x. ...</td>\n",
       "      <td>Les articles D. 842-15 à D. 842-18 ne sont pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4130</th>\n",
       "      <td>Code de la construction et de l'habitation</td>\n",
       "      <td>Code de la construction et de l'habitation.x. ...</td>\n",
       "      <td>Les articles  R.842-14,  R.843-2 à  R.843-8 ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>Code de la construction et de l'habitation</td>\n",
       "      <td>Code de la construction et de l'habitation.x. ...</td>\n",
       "      <td>Pour leur application à Saint-Pierre-et-Miquel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Source  \\\n",
       "0        Etablissement recevant des travailleurs   \n",
       "1        Etablissement recevant des travailleurs   \n",
       "2        Etablissement recevant des travailleurs   \n",
       "3        Etablissement recevant des travailleurs   \n",
       "4        Etablissement recevant des travailleurs   \n",
       "...                                          ...   \n",
       "4127  Code de la construction et de l'habitation   \n",
       "4128  Code de la construction et de l'habitation   \n",
       "4129  Code de la construction et de l'habitation   \n",
       "4130  Code de la construction et de l'habitation   \n",
       "4131  Code de la construction et de l'habitation   \n",
       "\n",
       "                                               Contexte  \\\n",
       "0     Etablissement recevant des travailleurs.x. Obl...   \n",
       "1     Etablissement recevant des travailleurs.x. Obl...   \n",
       "2     Etablissement recevant des travailleurs.x. Obl...   \n",
       "3     Etablissement recevant des travailleurs.x. Obl...   \n",
       "4     Etablissement recevant des travailleurs.x. Obl...   \n",
       "...                                                 ...   \n",
       "4127  Code de la construction et de l'habitation.x. ...   \n",
       "4128  Code de la construction et de l'habitation.x. ...   \n",
       "4129  Code de la construction et de l'habitation.x. ...   \n",
       "4130  Code de la construction et de l'habitation.x. ...   \n",
       "4131  Code de la construction et de l'habitation.x. ...   \n",
       "\n",
       "                                                Contenu  \n",
       "0     Les dispositions du présent titre déterminent,...  \n",
       "1     Pour l application du présent titre, on entend...  \n",
       "2     Le maître d ouvrage élabore et transmet aux ut...  \n",
       "3     Le dossier de maintenance des lieux de travail...  \n",
       "4     Le dossier de maintenance des lieux de travail...  \n",
       "...                                                 ...  \n",
       "4127  Pour leur application à Saint-Pierre-et-Miquel...  \n",
       "4128  Pour leur application à Saint-Pierre-et-Miquel...  \n",
       "4129  Les articles D. 842-15 à D. 842-18 ne sont pas...  \n",
       "4130  Les articles  R.842-14,  R.843-2 à  R.843-8 ne...  \n",
       "4131  Pour leur application à Saint-Pierre-et-Miquel...  \n",
       "\n",
       "[4132 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15cb44-8f34-472d-be80-92f4206c373b",
   "metadata": {},
   "source": [
    "# Recherche du texte le plus proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc580078-ea19-48f5-b954-2cd8cf2916fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: Je dois faire des modification dans un établissement recevant le public. Par qui l'autorisation est elle délivrée\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.5724\n",
      "\n",
      "Source plus proche:Code de la construction et de l'habitation\n",
      "\n",
      " Contexte: Code de la construction et de l'habitation.x. Article R641-19.x. Dispositions permettant de faire face à des difficultés particulières de logement..x. Mesures tendant à favoriser la construction d'habitations..x. Sociétés coopératives de construction..x. Agrément des contrôleurs techniques.x. Diagnostic portant sur les déchets issus de rénovations et de démolitions.x. Modifié par Décret n°99-340 du 29 avril 1999 - art. 1 () JORF 5 mai 1999.x. \n",
      "\n",
      "Texte: Le délai supplémentaire prévu àl article L.641-1, alinéa 4, ne peut être accordé au bénéficiaire de l attribution d office lorsque le propriétaire notifie qu il entre dans une des catégories prévues àl article L.641-2.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.5714\n",
      "\n",
      "Source plus proche:Code de la construction et de l'habitation\n",
      "\n",
      " Contexte: Code de la construction et de l'habitation.x. Article D331-79.x. Aides diverses à la construction d'habitations et à l'amélioration de l'habitat - Aide personnalisée au logement..x. Mesures tendant à favoriser la construction d'habitations..x. Sociétés coopératives de construction..x. Agrément des contrôleurs techniques.x. Diagnostic portant sur les déchets issus de rénovations et de démolitions.x. Modifié par Décret n°2019-873 du 21 août 2019 - art. 6.x. \n",
      "\n",
      "Texte: Les occupants des logements financés à l aide de ces prêts bénéficient de l aide personnalisée au logement dans les conditions prévues par le livre I II, titre V, du présent code (1re partie)  et de l article L.431-6.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.5429\n",
      "\n",
      "Source plus proche:Etablissement recevant des travailleurs\n",
      "\n",
      " Contexte: Etablissement recevant des travailleurs.x. Obligations du maître d'ouvrage pour la conception des lieux de travail  (Articles R4211-1 à R4217-2).x. Sécurité des lieux de travail (Articles R4214-1 à R4214-28).x. Aménagement des lieux et postes de travail (Articles R4214-22 à R4214-25).x. Article R4214-24.x. Créé par Décret n°2008-244 du 7 mars 2008 - art. (V).x. \n",
      "\n",
      "Texte: Si des postes de travail extérieurs sont prévus, ceux-ci sont conçus et aménagés suivant les prescriptions de l article R.4225-1.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "\n",
    "user_query = \"Je dois installer des dispositifs de désenfumage. A quoi dois je faire attention\"\n",
    "user_query = \"Je dois faire des modification dans un établissement recevant le public. Par qui l'autorisation est elle délivrée\"\n",
    "query_embeddings = get_embeddings(user_query)\n",
    "\n",
    "text_embeddings = text_embeddings.to(device)\n",
    "context_embeddings = context_embeddings.to(device)\n",
    "query_embeddings = query_embeddings.to(device)\n",
    "\n",
    "similarities = torch.nn.functional.cosine_similarity(query_embeddings, text_embeddings, dim=1)\n",
    "\n",
    "most_similar_index = torch.argmax(similarities).item()\n",
    "\n",
    "top_k = 3\n",
    "top_k_indices = torch.argsort(similarities, descending=True)[:top_k]\n",
    "print(f\"User query: {user_query}\\n\\n\")\n",
    "\n",
    "for idx in top_k_indices:\n",
    "    idx = idx.item() \n",
    "    text = df.loc[idx, 'Contenu']\n",
    "    context = df.loc[idx, 'Contexte']\n",
    "    source = df.loc[idx, 'Source']\n",
    "    similarity_coeff = similarities[idx]\n",
    "    print('-' * 120)\n",
    "    print(f\"Cosine Similarity: {similarity_coeff:.4f}\\n\\nSource plus proche:{df.loc[idx, 'Source']}\\n\\n Contexte: {df.loc[idx, 'Contexte']}\")\n",
    "    print(f\"\\nTexte: {df.loc[idx, 'Contenu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f351f8-0e22-45be-8bfe-af650b699cbf",
   "metadata": {},
   "source": [
    "# Finetuning espace d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a09e520-66a2-4b91-bf21-3a7c680cc36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Etablissement recevant des travailleurs',\n",
       "       'Réglementation sécurité et incendie. Etablissement recevant du public',\n",
       "       \"Règlement de sécurité contre les risques d'incendie et de panique. Etablissements recevant du public\",\n",
       "       \"Code de la construction et de l'habitation\"], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9d9a70-b42f-41d0-a241-b2eb98029749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset composé de: 3833 de relations positives (1) entre contexte et contenu\n",
      "Dataset composé de: 8000 de relations entre -1 et 1 entre contenus différents\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_1 = df[(df['Source']!=\"Code de la construction et de l'habitation\") & (df['Source']!=\"Etablissement recevant des travailleurs\")].copy()\n",
    "df_2 = df[df['Source']==\"Code de la construction et de l'habitation\"].copy()\n",
    "df_3 = df[df['Source']==\"Etablissement recevant des travailleurs\"].copy()\n",
    "\n",
    "\n",
    "df_1.reset_index(inplace=True, drop='True')\n",
    "df_2.reset_index(inplace=True, drop='True')\n",
    "\n",
    "def calculate_similarity(context_1, context_2):\n",
    "    context_1_list = context_1.split('.')[:-1]\n",
    "    context_2_list = context_2.split('.')[:-1]\n",
    "    common_levels =  len(list(set(context_1_list).intersection(context_2_list)))\n",
    "    max_levels = np.min([len(context_1_list), len(context_2_list)])\n",
    "    ratio = common_levels/max_levels\n",
    "    return -2 * np.exp(-ratio/0.2) + 1\n",
    "\n",
    "def get_index_list(df_, n):\n",
    "    lst_idx = []\n",
    "    for _ in range(n):\n",
    "        a, b = random.randint(0, len(df_)-1), random.randint(0, len(df_)-1)\n",
    "        while b == a: b = random.randint(0, len(df_)-1)\n",
    "        lst_idx.append((a, b))\n",
    "    return lst_idx\n",
    "\n",
    "list_1_indexes = get_index_list(df_=df_1, n=4000)\n",
    "liste_1 = [{\"sentence1\": df_1.loc[i, 'Contenu'], \"sentence2\": df_1.loc[j, 'Contenu'], \n",
    "            'label': calculate_similarity(df_1.loc[i, 'Contexte'], df_1.loc[j, 'Contexte'])} for i, j in list_1_indexes]\n",
    "\n",
    "list_2_indexes = get_index_list(df_=df_2, n=4000)\n",
    "liste_2 = [{\"sentence1\": df_2.loc[i, 'Contenu'], \"sentence2\": df_2.loc[j, 'Contenu'], \n",
    "            'label': calculate_similarity(df_2.loc[i, 'Contexte'], df_2.loc[i, 'Contexte'])} for i, j in list_2_indexes]\n",
    "\n",
    "data = [*[{\"sentence1\": df_1.loc[i, 'Contexte'], \"sentence2\": df_1.loc[i, 'Contenu'], 'label': 1} for i in range(len(df_1))],\n",
    "        *[{\"sentence1\": df_2.loc[i, 'Contexte'], \"sentence2\": df_2.loc[i, 'Contenu'], 'label': 1} for i in range(len(df_2))],\n",
    "        # *[{\"sentence1\": df_1.loc[i, 'Contexte'], \"sentence2\": df_2.loc[i, 'Contenu'], 'label': -1} for i in range(min(len(df_1), len(df_2)))],\n",
    "        # *[{\"sentence1\": df_2.loc[i, 'Contexte'], \"sentence2\": df_1.loc[i, 'Contenu'], 'label': -1} for i in range(min(len(df_1), len(df_2)))],\n",
    "        # *[{\"sentence1\": df_1.loc[i, 'Contexte'], \"sentence2\": df_2.loc[i, 'Contexte'], 'label': -1} for i in range(min(len(df_1), len(df_2)))],\n",
    "        # *[{\"sentence1\": df_2.loc[i, 'Contenu'], \"sentence2\": df_1.loc[i, 'Contenu'], 'label': -1} for i in range(min(len(df_1), len(df_2)))],\n",
    "        *liste_1, *liste_2]\n",
    "\n",
    "print(f'Dataset composé de: {len(df_1) + len(df_2)} de relations positives (1) entre contexte et contenu')\n",
    "print(f'Dataset composé de: {len(liste_1) + len(liste_2)} de relations entre -1 et 1 entre contenus différents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ed795-f18d-4d96-b177-9cb2a9ea7fe2",
   "metadata": {},
   "source": [
    "# Construction de la bade de donnees avec label contrastif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70737615-65fe-469a-9d41-ded3b7094784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "        tokens = self.tokenizer(row['sentence1'], row['sentence2'], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        label = torch.tensor(row['label'], dtype=torch.float)\n",
    "        return tokens, label\n",
    "\n",
    "tokenizer = globals()[f'{model_suffix}Tokenizer'].from_pretrained(model_name)\n",
    "dataset = ContrastiveDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf283bf-9270-49de-b6f1-d7417dca9af3",
   "metadata": {},
   "source": [
    "# Définition du model: XBert + couche de projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67b396ec-eee6-47d2-8932-bdb065f4d631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ContrastiveModel(\n",
       "  (bert): FlaubertModel(\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (embeddings): Embedding(68729, 768, padding_idx=2)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, model_name=model_name, embedding_dim=text_embeddings.shape[1]):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.bert = globals()[f'{model_suffix}Model'].from_pretrained(model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0] \n",
    "        return self.projection(embeddings)\n",
    "\n",
    "model_C = ContrastiveModel()\n",
    "model_C.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394d6d9-43fe-4e01-a859-21634e78232b",
   "metadata": {},
   "source": [
    "# Définition de la fonction perte: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08308559-566d-4066-9d76-ab99a202374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(embeddings1, embeddings2, labels, margin=1.0):\n",
    "    cosine_sim = F.cosine_similarity(embeddings1, embeddings2)\n",
    "    loss = torch.mean((cosine_sim - labels) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8b963-bb89-40c9-ad86-dacf12c97cdd",
   "metadata": {},
   "source": [
    "# Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf6eaced-297a-47ca-b607-1b97abe1cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2444411665201187\n",
      "Epoch 2 Loss: 0.03072589635848999\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "n_epochs = 2\n",
    "for epoch in range(n_epochs):\n",
    "    model_C.train()\n",
    "    for batch in dataloader:\n",
    "        tokens, labels = batch\n",
    "        input_ids = tokens['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeddings1 = model_C(input_ids, attention_mask)\n",
    "        embeddings2 = model_C(input_ids, attention_mask)\n",
    "\n",
    "        loss = contrastive_loss(embeddings1, embeddings2, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a649e-0389-4808-a1f2-5c96630cfb21",
   "metadata": {},
   "source": [
    "# Construction matrice de similarité nouveaux embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2833abe-65be-449c-8da6-c4caedc613ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 4132: 100%|█████████████████████████████████████████████████████████████| 4132/4132 [01:04<00:00, 64.35it/s]\n",
      "Text embeddings done.\n",
      "Processed: 4132: 100%|█████████████████████████████████████████████████████████████| 4132/4132 [01:05<00:00, 62.67it/s]\n",
      "Context embeddings done.\n",
      "Cosine similarities calculated.\n"
     ]
    }
   ],
   "source": [
    "def embed_text(query, model, tokenizer, device):\n",
    "    model_C.eval()\n",
    "    tokens = tokenizer(query, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model_C(tokens['input_ids'], tokens['attention_mask'])\n",
    "    embeddings = output[0]\n",
    "    if len(embeddings.shape) == 1:\n",
    "        return embeddings  \n",
    "    else:\n",
    "        return embeddings.mean(dim=1)  \n",
    " \n",
    "\n",
    "def process_embeddings(data, model_, tokenizer, device):\n",
    "    embeddings = []\n",
    "    with tqdm(total=len(data), file=sys.stdout) as prog_bar:\n",
    "        for i in range(len(data)):\n",
    "            embeddings.append(embed_text(data[i], model_, tokenizer, device))\n",
    "            prog_bar.set_description('Processed: %d' % (i + 1))\n",
    "            prog_bar.update(1)\n",
    "    return torch.stack(embeddings)  \n",
    "\n",
    "query_embeddings = embed_text(user_query, model, tokenizer, device)\n",
    "query_embeddings = query_embeddings.to(device)\n",
    "\n",
    "text_embeddings_finetuned = process_embeddings(texts_, model_C, tokenizer, device)\n",
    "print(\"Text embeddings done.\")\n",
    "\n",
    "context_embeddings_finetuned = process_embeddings(contexts, model_C, tokenizer, device)\n",
    "print(\"Context embeddings done.\")\n",
    "\n",
    "\n",
    "similarities = torch.nn.functional.cosine_similarity(query_embeddings, text_embeddings_finetuned, dim=1)\n",
    "print(\"Cosine similarities calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfcf7463-c129-4b34-bc82-5dabde9b5d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query: Je dois faire des modification dans un établissement recevant le public. Par qui l'autorisation est elle délivrée\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.6734\n",
      "\n",
      "Source plus proche:Code de la construction et de l'habitation\n",
      "\n",
      " Contexte: Code de la construction et de l'habitation.x. Article R311-55.x. Aides diverses à la construction d'habitations et à l'amélioration de l'habitat - Aide personnalisée au logement..x. Mesures tendant à favoriser la construction d'habitations..x. Sociétés coopératives de construction..x. Agrément des contrôleurs techniques.x. Diagnostic portant sur les déchets issus de rénovations et de démolitions.x. Primes convertibles et prêts spéciaux destinés au financement d'immeubles à loyer moyen..x. Modifié par Décret n°2019-873 du 21 août 2019 - art. 4.x. \n",
      "\n",
      "Texte: Les logements doivent être loués nus par bail écrit.  Les conditions auxquelles les baux doivent satisfaire sont fixées par l arrêté prévu àl article  R.311-52;  elles sont rappelées dans le contrat de prêt.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.6572\n",
      "\n",
      "Source plus proche:Code de la construction et de l'habitation\n",
      "\n",
      " Contexte: Code de la construction et de l'habitation.x. Article R261-27.x. Statut des constructeurs..x. Ventes d'immeubles à construire ou à rénover..x. Sociétés coopératives de construction..x. Agrément des contrôleurs techniques.x. Diagnostic portant sur les déchets issus de rénovations et de démolitions.x. Modifié par Décret n°2019-873 du 21 août 2019 - art. 4.x. \n",
      "\n",
      "Texte: Le contrat préliminaire est établi par écrit ;  un exemplaire doit en être remis au réservataire avant tout dépôt de fonds.  Il doit obligatoirement reproduire les dispositions desarticles  R.261-28 à  R.261-31.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Cosine Similarity: 0.6540\n",
      "\n",
      "Source plus proche:Etablissement recevant des travailleurs\n",
      "\n",
      " Contexte: Etablissement recevant des travailleurs.x. Obligations de l'employeur pour l'utilisation des lieux de travail  (Articles R4221-1 à R4228-37).x. Éclairage, ambiance thermique (Articles R4223-1 à R4223-15).x. Éclairage (Articles R4223-1 à R4223-12).x. Article R4223-2.x. Créé par Décret n°2008-244 du 7 mars 2008 - art. (V).x. \n",
      "\n",
      "Texte: L éclairage est assuré de manière à : 1°  Eviter la fatigue visuelle et les affections de la vue qui en résultent ; 2°  Permettre de déceler les risques perceptibles par la vue.\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_k_indices = torch.argsort(similarities, descending=True)[:top_k]\n",
    "print(f\"User query: {user_query}\\n\\n\")\n",
    "\n",
    "for idx in top_k_indices:\n",
    "    idx = idx.item() \n",
    "    text = df.loc[idx, 'Contenu']\n",
    "    context = df.loc[idx, 'Contexte']\n",
    "    source = df.loc[idx, 'Source']\n",
    "    similarity_coeff = similarities[idx]\n",
    "    print('-' * 120)\n",
    "    print(f\"Cosine Similarity: {similarity_coeff:.4f}\\n\\nSource plus proche:{df.loc[idx, 'Source']}\\n\\n Contexte: {df.loc[idx, 'Contexte']}\")\n",
    "    print(f\"\\nTexte: {df.loc[idx, 'Contenu']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22e9cd-0b89-4031-b48e-2e322263d272",
   "metadata": {},
   "source": [
    "# Evaluation des transformations de l'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2bd20f1-1ae3-439e-bf84-74041404e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "    \n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "\n",
    "def embed_text_(query, model, tokenizer, device):\n",
    "    model_C.eval()\n",
    "    tokens = tokenizer(query, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model_C(tokens['input_ids'], tokens['attention_mask'])\n",
    "    return output[0].squeeze(0)\n",
    "\n",
    "list_mots = [mot for mot in re.split(r\"[^\\w']+\", ' '.join(texts_).lower()) if (mot.lower() not in stop_words) & (len(mot)>3)]\n",
    "    \n",
    "comptage_mots = Counter(list_mots)\n",
    "mots_plus_frequents = [mot for mot, _ in comptage_mots.most_common(10)]\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_avant_2d = pca.fit_transform(text_embeddings.cpu().numpy())\n",
    "embeddings_apres_2d = pca.transform(text_embeddings_finetuned.cpu().numpy())\n",
    "\n",
    "mots_plus_frequents_embeddings_avant = []\n",
    "mots_plus_frequents_embeddings_apres = []\n",
    "for mot in mots_plus_frequents:\n",
    "    mot_embedding_avant = get_embeddings(mot)\n",
    "    mot_embedding_apres = embed_text_(mot, model_C, tokenizer, device)\n",
    "    \n",
    "    mots_plus_frequents_embeddings_avant.append(mot_embedding_avant)\n",
    "    mots_plus_frequents_embeddings_apres.append(mot_embedding_apres)\n",
    "\n",
    "mots_plus_frequents_embeddings_avant = torch.cat(mots_plus_frequents_embeddings_avant, dim=0)\n",
    "mots_plus_frequents_embeddings_apres = torch.cat(mots_plus_frequents_embeddings_apres, dim=0)\n",
    "\n",
    "embeddings_plus_frequent_avant_2d = pca.transform(mots_plus_frequents_embeddings_avant.cpu().numpy())\n",
    "embeddings_plus_frequent_apres_2d = pca.transform(mots_plus_frequents_embeddings_apres.cpu().numpy().reshape(10, 768))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(embeddings_plus_frequent_avant_2d[:, 0], embeddings_plus_frequent_avant_2d[:, 1], color='blue', label='Avant finetuning')\n",
    "plt.scatter(embeddings_plus_frequent_apres_2d[:, 0], embeddings_plus_frequent_apres_2d[:, 1], color='red', label='Après finetuning')\n",
    "\n",
    "for i, mot in enumerate(mots_plus_frequents):\n",
    "    plt.annotate(mot, (embeddings_plus_frequent_avant_2d[i, 0], embeddings_plus_frequent_avant_2d[i, 1]), color='blue')\n",
    "    plt.annotate(mot, (embeddings_plus_frequent_apres_2d[i, 0], embeddings_plus_frequent_apres_2d[i, 1]), color='red')\n",
    "\n",
    "plt.xlabel('Composante principale 1')\n",
    "plt.ylabel('Composante principale 2')\n",
    "plt.title('Comparaison des embeddings avant et après finetuning avec PCA')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74dbf81d-c598-434a-98e6-d5c40f17e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ouvrages', 'ouvrage'}\n"
     ]
    }
   ],
   "source": [
    "recherche = 'ouvrage'\n",
    "print(set([item for item in list_mots if recherche in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71eb5c5b-849d-42fd-9b42-e2a12546a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5c179d6-99ac-4a41-bab0-15292efd6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mots_plus_frequents_embeddings_apres))\n",
    "type(mots_plus_frequents_embeddings_apres[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f2701-6318-4dd7-a7f1-8053eed1f6cb",
   "metadata": {},
   "source": [
    "# Enregistrement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0391b0c2-806a-4e75-b931-e44b0ac4296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "config.save_pretrained('./finetuned/model')\n",
    "\n",
    "tokenizer.save_pretrained('./finetuned/model')\n",
    "torch.save(model.state_dict(), './finetuned/contrastive_model.pth')\n",
    "\n",
    "df_embeddings = pd.DataFrame({\n",
    "    'Text': [*texts_, *contexts],\n",
    "    'Embedding': [embedding.tolist() for embedding in (*text_embeddings.to('cpu'), *context_embeddings.to('cpu'))], \n",
    "})\n",
    "df_embeddings.to_csv('./finetuned/embedding_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db26dfc-05bb-405c-8a38-f80f8a79d40e",
   "metadata": {},
   "source": [
    "# Mise en place du RAG:\n",
    "- Utilisation de faiss alimenté par l'embedding réentrainé pour le retriver\n",
    "- Utilisation de AutoModelForSeq2SeqLM \"t5-base\" por le generateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb543d5b-ff61-45f0-93b3-9adb2b09aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index construit avec 8264 documents.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "import ast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./finetuned/model')\n",
    "model_loaded = ContrastiveModel()  \n",
    "model_loaded.load_state_dict(torch.load('./finetuned/contrastive_model.pth'), strict=False)\n",
    "model_loaded.eval().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "df_embeddings = pd.read_csv('./finetuned/embedding_matrix.csv')\n",
    "df_embeddings['Embedding'] = df_embeddings['Embedding'].apply(ast.literal_eval)\n",
    "texts = df_embeddings['Text'].tolist()\n",
    "embeddings = torch.tensor(df_embeddings['Embedding'].tolist())\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1]) \n",
    "index.add(embeddings.numpy()) \n",
    "\n",
    "print(f\"Index construit avec {index.ntotal} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2107b007-0692-4cf6-8416-f2381203ee40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Connection error caused by: ConnectionError(Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:289\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elastic_transport\\_node\\_http_urllib3.py:167\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[1;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[0;32m    165\u001b[0m     body_to_send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    168\u001b[0m     method,\n\u001b[0;32m    169\u001b[0m     target,\n\u001b[0;32m    170\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody_to_send,\n\u001b[0;32m    171\u001b[0m     retries\u001b[38;5;241m=\u001b[39mRetry(\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    172\u001b[0m     headers\u001b[38;5;241m=\u001b[39mrequest_headers,\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m HttpHeaders(response\u001b[38;5;241m.\u001b[39mheaders)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\util\\retry.py:449\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m error:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\http\\client.py:289\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m es \u001b[38;5;241m=\u001b[39m Elasticsearch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:9200\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      9\u001b[0m     es\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mcreate(index\u001b[38;5;241m=\u001b[39mindex_name, body\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmappings\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     10\u001b[0m                                                            {\n\u001b[0;32m     11\u001b[0m                                                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[0;32m     12\u001b[0m                                                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m: embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]},\n\u001b[0;32m     13\u001b[0m                                                            }}})\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_es_data\u001b[39m(texts, embeddings):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elasticsearch\\_sync\\client\\utils.py:455\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m api(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elasticsearch\\_sync\\client\\indices.py:1299\u001b[0m, in \u001b[0;36mIndicesClient.exists\u001b[1;34m(self, index, allow_no_indices, error_trace, expand_wildcards, filter_path, flat_settings, human, ignore_unavailable, include_defaults, local, pretty)\u001b[0m\n\u001b[0;32m   1297\u001b[0m     __query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretty\n\u001b[0;32m   1298\u001b[0m __headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m-> 1299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices.exists\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__path_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elasticsearch\\_sync\\client\\_base.py:423\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[1;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mperform_request\u001b[39m(\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    412\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elasticsearch\\_sync\\client\\_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[1;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mperform_request\u001b[39m(\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[0;32m    267\u001b[0m         method,\n\u001b[0;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[0;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[1;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elasticsearch\\_sync\\client\\_base.py:316\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[1;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     target \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m--> 316\u001b[0m meta, resp_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_on_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m299\u001b[39m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     )\n\u001b[0;32m    338\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elastic_transport\\_transport.py:342\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[1;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta, otel_span)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     otel_span\u001b[38;5;241m.\u001b[39mset_node_metadata(node\u001b[38;5;241m.\u001b[39mhost, node\u001b[38;5;241m.\u001b[39mport, node\u001b[38;5;241m.\u001b[39mbase_url, target)\n\u001b[1;32m--> 342\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m [status:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m duration:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m         )\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\elastic_transport\\_node\\_http_urllib3.py:202\u001b[0m, in \u001b[0;36mUrllib3HttpNode.perform_request\u001b[1;34m(self, method, target, body, headers, request_timeout)\u001b[0m\n\u001b[0;32m    194\u001b[0m         err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e), errors\u001b[38;5;241m=\u001b[39m(e,))\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(\n\u001b[0;32m    196\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    197\u001b[0m         target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m         exception\u001b[38;5;241m=\u001b[39merr,\n\u001b[0;32m    201\u001b[0m     )\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    204\u001b[0m meta \u001b[38;5;241m=\u001b[39m ApiResponseMeta(\n\u001b[0;32m    205\u001b[0m     node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    206\u001b[0m     duration\u001b[38;5;241m=\u001b[39mduration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresponse_headers,\n\u001b[0;32m    210\u001b[0m )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_request(\n\u001b[0;32m    212\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    213\u001b[0m     target\u001b[38;5;241m=\u001b[39mtarget,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m     response\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    218\u001b[0m )\n",
      "\u001b[1;31mConnectionError\u001b[0m: Connection error caused by: ConnectionError(Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))))"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "index_name = \"documents\"\n",
    "\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body={\"mappings\": {\"properties\":\n",
    "                                                           {\n",
    "                                                               \"Text\": {\"type\": \"text\"}, \n",
    "                                                               \"Embedding\": {\"type\": \"dense_vector\", \"dims\": embeddings.shape[1]},\n",
    "                                                           }}})\n",
    "\n",
    "def generate_es_data(texts, embeddings):\n",
    "    for idx, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "        yield {\"_index\": index_name,\n",
    "               \"_id\": idx,\n",
    "               \"_source\": {\"Text\": text, \"Embedding\": embedding.tolist(),}\n",
    "              }\n",
    "\n",
    "bulk(es, generate_es_data(texts, embeddings.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ec015-42c8-4816-b945-179a64e39ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_elasticsearch(query, model, tokenizer, es, index_name, top_k=5):\n",
    "    query_embedding = embed_text(query, model, tokenizer, 'cpu').tolist()\n",
    "    body = {\"size\": top_k,\n",
    "            \"query\": {\"script_score\": {\"query\": {\"match_all\": {}}, \n",
    "                                       \"script\": {\"source\": \"cosineSimilarity(params.query_vector, 'Embedding') + 1.0\", \n",
    "                                                  \"params\": {\"query_vector\": query_embedding}}\n",
    "                                      }\n",
    "                     }\n",
    "           }\n",
    "    response = es.search(index=index_name, body=body)\n",
    "    results = [{\"text\": hit[\"_source\"][\"Text\"], \"score\": hit[\"_score\"]} for hit in response[\"hits\"][\"hits\"]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "497f0906-9967-4d56-9384-594a34749a2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_answer(context, user_query, generator, tokenizer)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRéponse générée :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[84], line 22\u001b[0m, in \u001b[0;36mrag_pipeline\u001b[1;34m(user_query, model, tokenizer, index, texts, generator)\u001b[0m\n\u001b[0;32m     19\u001b[0m retrieved \u001b[38;5;241m=\u001b[39m retrieve(user_query, model, tokenizer, index, texts, es, index_name, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     20\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m retrieved])\n\u001b[1;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[1;32mIn[84], line 13\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(context, query, generator, tokenizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\transformers\\generation_utils.py:922\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m    918\u001b[0m encoder_input_ids \u001b[38;5;241m=\u001b[39m input_ids \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;66;03m# add encoder_outputs to model_kwargs\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;66;03m# set input_ids as decoder_input_ids\u001b[39;00m\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\transformers\\generation_utils.py:417\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, input_ids, model_kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_encoder()\n\u001b[0;32m    412\u001b[0m     encoder_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    413\u001b[0m         argument: value\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m argument, value \u001b[38;5;129;01min\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (argument\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m argument\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    416\u001b[0m     }\n\u001b[1;32m--> 417\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(input_ids, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:896\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 896\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# required mask seq length can be calculated via length of past\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_pytorch\\lib\\site-packages\\torch\\nn\\functional.py:2044\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "def retrieve(query, model, tokenizer, index, texts, es, index_name, top_k=5):\n",
    "    query_embedding = embed_text(query, model, tokenizer, device).unsqueeze(0)\n",
    "    distances, indices = index.search(query_embedding.to('cpu').numpy(), top_k)\n",
    "    fais_results = [{\"text\": texts[i], \"distance\": distances[0][j]} for j, i in enumerate(indices[0])]\n",
    "    return fais_results\n",
    "\n",
    "def generate_answer(context, query, generator, tokenizer):\n",
    "    input_text = f\"Context: {context} Query: {query}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = generator.generate(input_ids, max_length=128, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "generator = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\").to('cpu')\n",
    "\n",
    "def rag_pipeline(user_query, model, tokenizer, index, texts, generator):\n",
    "    retrieved = retrieve(user_query, model, tokenizer, index, texts, es, index_name, top_k=5)\n",
    "    context = \" \".join([item['text'] for item in retrieved])\n",
    "\n",
    "    response = generate_answer(context, user_query, generator, tokenizer)\n",
    "    return response\n",
    "\n",
    "response = rag_pipeline(user_query, model, tokenizer, index, texts, generator)\n",
    "print(\"Réponse générée :\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2325960-a152-4aae-a4c6-04df15689bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.chains import RAGChain, LLMChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# class FAISS_Retriever:\n",
    "#     def __init__(self, index, texts):\n",
    "#         self.index = index\n",
    "#         self.texts = texts\n",
    "\n",
    "#     def retrieve(self, query, top_k=3):\n",
    "#         inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#         with torch.no_grad():\n",
    "#             query_embedding = generator(**inputs).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "#         distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "#         results = [self.texts[i] for i in indices[0]]\n",
    "#         return results\n",
    "\n",
    "# retriever = FAISS_Retriever(index, texts)\n",
    "\n",
    "# template = \"\"\"\n",
    "# Voici le contexte de la demande:  {context}\n",
    "# Dans ce cadre, réponds à cette question: {query}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
    "# llm = HuggingFacePipeline(pipeline=generator)\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# retrieved_docs = retriever.retrieve(user_query)\n",
    "# context = \" \".join(retrieved_docs)\n",
    "# response = chain.run({\"context\": context, \"query\": query})\n",
    "\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_pytorch)",
   "language": "python",
   "name": "gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
